By Neil Mclsaac

https://esi.learnondemand.net/TrainingKey/Redeem
https://learn.microsoft.com/en-us/training/courses/ai-050t00?wt.mc_id=esi_m2l_content_wwl#study-guide

[9:39 AM] Neil McIsaac
https://aka.ms/esilabs

Training Key: 24B2996E698044F8

 like 1
Login - Skillable TMS

### LLMs - Encoder and Attention
use this prompt in your copilot: explain how GPT know that dog and cat are somewhat close in a vector space. ow does  was it trained and which process to make that determination i vector space?  This is the response: Sure, I’d be happy to explain!
GPT, or Generative Pretrained Transformer, is a type of language model that uses machine learning to generate human-like text. It’s trained on a large corpus of text data and learns to predict the next word in a sentence.
The way GPT understands that “dog” and “cat” are somewhat close in a vector space is through a process called word embedding. Word embedding is a technique where words or phrases from the vocabulary are mapped to vectors of real numbers in a lower-dimensional space relative to the vocabulary size. These vectors capture semantic and syntactic meanings of the words.
During training, GPT uses a method called unsupervised learning. It’s fed with sentences and learns to predict the next word in a sentence. For example, in the sentence “The dog chased its ___”, the model learns that words like “tail” or “ball” are likely to fill the blank. Over time, through billions of such sentences, the model starts to understand the context and meaning of words.
In the vector space, words that are used and appear in similar contexts tend to be closer to each other. So, “dog” and “cat” would be closer in this space because they both are pets, have tails, can be domesticated, etc., and these facts are often discussed in similar contexts.
It’s important to note that GPT doesn’t “understand” text in the way humans do. It doesn’t have a concept of a “dog” or a “cat” beyond what patterns it has learned from its training data.

###
