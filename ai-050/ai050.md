By Neil Mclsaac

https://esi.learnondemand.net/TrainingKey/Redeem
https://learn.microsoft.com/en-us/training/courses/ai-050t00?wt.mc_id=esi_m2l_content_wwl#study-guide

[9:39 AM] Neil McIsaac
https://aka.ms/esilabs

Training Key: 24B2996E698044F8

 like 1
Login - Skillable TMS

Azure Portal
URL	https://portal.azure.com/#home
Subscription	5160a060-a769-4747-bd10-d7e1a2cc27a4
Username	LabUser-38984925@cloudslice.onmicrosoft.com
Password	Nq0Se8+!Dr
Resource Group
RG1-lod38984925
Thumbnail screenshot of virtual machineLab38984925-AI-050-VM


### LLMs - Encoder and Attention
use this prompt in your copilot: explain how GPT know that dog and cat are somewhat close in a vector space. ow does  was it trained and which process to make that determination i vector space?  This is the response: Sure, I’d be happy to explain!
GPT, or Generative Pretrained Transformer, is a type of language model that uses machine learning to generate human-like text. It’s trained on a large corpus of text data and learns to predict the next word in a sentence.
The way GPT understands that “dog” and “cat” are somewhat close in a vector space is through a process called word embedding. Word embedding is a technique where words or phrases from the vocabulary are mapped to vectors of real numbers in a lower-dimensional space relative to the vocabulary size. These vectors capture semantic and syntactic meanings of the words.
During training, GPT uses a method called unsupervised learning. It’s fed with sentences and learns to predict the next word in a sentence. For example, in the sentence “The dog chased its ___”, the model learns that words like “tail” or “ball” are likely to fill the blank. Over time, through billions of such sentences, the model starts to understand the context and meaning of words.
In the vector space, words that are used and appear in similar contexts tend to be closer to each other. So, “dog” and “cat” would be closer in this space because they both are pets, have tails, can be domesticated, etc., and these facts are often discussed in similar contexts.
It’s important to note that GPT doesn’t “understand” text in the way humans do. It doesn’t have a concept of a “dog” or a “cat” beyond what patterns it has learned from its training data.

###
The number of dimensions used to vectorize words can vary, but it’s typically in the hundreds. For example, the popular Word2Vec model often uses 300 dimensions1. Each dimension in these vectors can be thought of as representing some aspect of the word’s meaning3.
These high-dimensional vectors capture relationships between words. Words that appear in similar contexts are mapped to vectors that are nearby in this high-dimensional space4. This allows the model to capture semantic relationships between words, such as synonyms and analogies1.
So, while the exact number can vary depending on the specific model and configuration, it’s common to use several hundred dimensions when vectorizing words5.

### Codex / CoPilot
OpenAI Codex and GPT are both powerful AI systems, but they have different capabilities and uses. Codex is designed for understanding and generating code, while GPT is designed for understanding and generating natural language.

Copilots are Microsoft implementation of OpenAI GPT. It is an enhancement and augmentation.  We use RAG and one of the key RAG elements is Microsoft Graph. The free version of Microsoft Copilot can be accessed and used at https://copilot.microsoft.com

### What is Azure OpenAI
https://learn.microsoft.com/en-us/azure/ai-services/openai/overview



















